#  SymbioTech – AI-Driven Smart Assistive Ecosystem

##  Abstract
The Smart Assistive Ecosystem Project aims to create an inclusive and intelligent communication and navigation system that empowers individuals with visual, hearing, and speech impairments.  
The ecosystem integrates three core components — **Smart Glasses**, **Smart Gloves**, and a **Mobile Application** — to facilitate seamless interaction and independence.

- The **Smart Glasses** assist blind users by reading text aloud, recognizing objects and faces, and providing navigation guidance.  
- The **Smart Gloves** enable deaf and mute users to communicate by translating sign language gestures into speech or text and converting spoken input into tactile or visual feedback.  
- Acting as the central hub, the **Mobile Application** connects both devices, managing translation, communication, and personalization features.  

Through AI integration, emotion detection, and cloud connectivity, the system bridges communication gaps between different disabilities while enhancing accessibility, independence, and social inclusion.

---

##  Problem Statement 
Millions of individuals worldwide face daily challenges due to visual, hearing, or speech impairments. These limitations hinder their ability to communicate effectively, navigate safely, and live independently.  
Current assistive tools often focus on one disability at a time and fail to integrate communication between different impairment groups.  

Our motivation is to create an **all-in-one system** that bridges this gap, enabling real-time communication and navigation through intelligent, affordable, and accessible technology.  
The project primarily impacts people with disabilities, their families, and institutions supporting inclusive communities.

---

##  Proposed Solution
The **Smart Assistive Ecosystem (SymbioTech)** offers an integrated, AI-driven solution that directly addresses the communication and navigation challenges faced by individuals with visual, hearing, and speech impairments.  
Unlike existing assistive tools that focus on a single disability, **SymbioTech** unites multiple assistive functions into one intelligent ecosystem.

###  Smart Glasses
Designed for visually impaired users, the glasses utilize a built-in camera, sensors, and AI algorithms to recognize text, objects, faces, and surroundings.  
They can read printed text aloud and provide real-time audio feedback to assist in navigation and object identification.  
Optional GPS integration enhances mobility and safety, while cloud-based AI and emotion detection improve user interaction — transforming the glasses into a personal assistant rather than a simple aid.

###  Smart Gloves
The gloves assist deaf and mute individuals by translating hand gestures into speech or text using flex sensors, accelerometers, and a microcontroller.  
They transmit gesture data via Bluetooth or Wi-Fi to the mobile app for instant translation.  
The gloves also receive spoken input, converting it into vibrations or text for two-way communication, enabling meaningful interaction between hearing and visually impaired users.

###  Mobile Application
The app acts as the control center linking both devices. It processes data, manages translation between gesture, text, and speech, and provides customizable communication modes.  
It includes real-time chat, navigation support, voice-to-text and text-to-speech conversion, and personalization settings.  
Through AI integration and cloud connectivity, the app ensures smooth synchronization, accessibility, and scalability.

---

##  Key Features
-  AI-based text, object, and face recognition  
-  Gesture-to-speech and speech-to-text translation  
-  Two-way communication with real-time feedback  
-  Customizable interface and user preferences  
-  Cross-platform accessibility via mobile application  

---

## Technology Stack
| Category | Tools / Technologies |
|-----------|----------------------|
| **Microcontroller** | ESP32-CAM |
| **Sensors** | Ultrasonic, Accelerometer, Flex Sensors, LiDAR |
| **Programming Languages** | Python, C++, JavaScript |
| **Frameworks** | TensorFlow Lite, Flask, Android Studio |
| **Database / Cloud** | Firebase |

These technologies are **open-source, low-cost, and optimized** for IoT and AI integration — suitable for real-time assistive applications and efficient embedded operation.

---

## Success Metrics
- **Functional Accuracy:** ≥90% accuracy in object detection and gesture recognition  
- **User Testing:** ≥80% positive user feedback from trial participants  
- **System Reliability:** Stable wireless connectivity between glasses, gloves, and mobile app  

---

## License
This project is licensed under the [MIT License](LICENSE).

---

## Contact
For inquiries or collaboration:
- **Hana Amr** – s-hana.mohamed@zewailcity.edu.eg  
- **Rudaina Haitham** – s-rudaina.salmin@zewailcity.edu.eg  
- **Youssef Ahmed Abdelmawla** – s-youssef.elmawla@zewailcity.edu.eg
