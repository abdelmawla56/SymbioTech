# ü§ñ SymbioTech ‚Äì AI-Driven Smart Assistive Ecosystem

## üß≠ Abstract
The Smart Assistive Ecosystem Project aims to create an inclusive and intelligent communication and navigation system that empowers individuals with visual, hearing, and speech impairments.  
The ecosystem integrates three core components ‚Äî **Smart Glasses**, **Smart Gloves**, and a **Mobile Application** ‚Äî to facilitate seamless interaction and independence.

- The **Smart Glasses** assist blind users by reading text aloud, recognizing objects and faces, and providing navigation guidance.  
- The **Smart Gloves** enable deaf and mute users to communicate by translating sign language gestures into speech or text and converting spoken input into tactile or visual feedback.  
- Acting as the central hub, the **Mobile Application** connects both devices, managing translation, communication, and personalization features.  

Through AI integration, emotion detection, and cloud connectivity, the system bridges communication gaps between different disabilities while enhancing accessibility, independence, and social inclusion.

---

## üí° Problem Statement & Motivation
Millions of individuals worldwide face daily challenges due to visual, hearing, or speech impairments. These limitations hinder their ability to communicate effectively, navigate safely, and live independently.  
Current assistive tools often focus on one disability at a time and fail to integrate communication between different impairment groups.  

Our motivation is to create an **all-in-one system** that bridges this gap, enabling real-time communication and navigation through intelligent, affordable, and accessible technology.  
The project primarily impacts people with disabilities, their families, and institutions supporting inclusive communities.

---

## üß† Proposed Solution
The **Smart Assistive Ecosystem (SymbioTech)** offers an integrated, AI-driven solution that directly addresses the communication and navigation challenges faced by individuals with visual, hearing, and speech impairments.  
Unlike existing assistive tools that focus on a single disability, **SymbioTech** unites multiple assistive functions into one intelligent ecosystem.

### üîπ Smart Glasses
Designed for visually impaired users, the glasses utilize a built-in camera, sensors, and AI algorithms to recognize text, objects, faces, and surroundings.  
They can read printed text aloud and provide real-time audio feedback to assist in navigation and object identification.  
Optional GPS integration enhances mobility and safety, while cloud-based AI and emotion detection improve user interaction ‚Äî transforming the glasses into a personal assistant rather than a simple aid.

### üîπ Smart Gloves
The gloves assist deaf and mute individuals by translating hand gestures into speech or text using flex sensors, accelerometers, and a microcontroller.  
They transmit gesture data via Bluetooth or Wi-Fi to the mobile app for instant translation.  
The gloves also receive spoken input, converting it into vibrations or text for two-way communication, enabling meaningful interaction between hearing and visually impaired users.

### üîπ Mobile Application
The app acts as the control center linking both devices. It processes data, manages translation between gesture, text, and speech, and provides customizable communication modes.  
It includes real-time chat, navigation support, voice-to-text and text-to-speech conversion, and personalization settings.  
Through AI integration and cloud connectivity, the app ensures smooth synchronization, accessibility, and scalability.

---

## ‚ú® Key Features
- üß† AI-based text, object, and face recognition  
- ü§ù Gesture-to-speech and speech-to-text translation  
- üîÑ Two-way communication with real-time feedback  
- ‚öôÔ∏è Customizable interface and user preferences  
- üì± Cross-platform accessibility via mobile application  

---

## üéØ Project Scope

### ‚úÖ In Scope
The Smart Assistive Ecosystem focuses on designing, developing, and integrating three core components to create an inclusive assistive system:

- **Smart Glasses:** Prototype capable of text reading, object recognition, and basic obstacle detection using AI and ultrasonic sensors.  
- **Smart Gloves:** Functional prototype to detect hand gestures and convert them into speech or text in real time using flex sensors and a microcontroller.  
- **Mobile Application:** Central communication hub connecting both devices via Bluetooth or Wi-Fi for real-time data exchange, gesture translation, and voice/text conversion.  
- **AI Integration:** Implementation of machine and deep learning models for text and object recognition, and voice/text translation.  
- **User Interface:** Simple and accessible mobile app interface allowing customization and real-time interaction.  
- **Testing & Evaluation:** Assess communication accuracy, response time, and user experience.  

### üö´ Out of Scope
To maintain focus and manage development efforts, the following are excluded from this phase:
- Full-scale commercial production or mass manufacturing  
- Integration with third-party assistive systems or medical databases  
- High-precision GPS navigation (basic only)  
- Cloud-based large-scale storage or deployment  
- Long-term clinical or medical-grade certification  

---

## üóìÔ∏è High-Level Timeline
| Phase | Description | Duration |
|--------|--------------|-----------|
| Research & Requirements | Literature review, market analysis, technology selection | 2 weeks |
| Design | System architecture, UI/UX, and hardware schematics | 2 weeks |
| Implementation 1 | Hardware development (smart glasses & gloves) | 4 weeks |
| Implementation 2 | Software integration and mobile app | 4 weeks |
| Testing | Unit and usability testing | 2 weeks |
| Integration | Hardware‚Äìsoftware synchronization | 2 weeks |
| Final Report & Presentation | Documentation and demo preparation | 2 weeks |

---

## üß∞ Technology Stack
| Category | Tools / Technologies |
|-----------|----------------------|
| **Microcontroller** | ESP32-CAM |
| **Sensors** | Ultrasonic, Accelerometer, Flex Sensors, LiDAR |
| **Programming Languages** | Python, C++, JavaScript |
| **Frameworks** | TensorFlow Lite, Flask, Android Studio |
| **Database / Cloud** | Firebase |

These technologies are **open-source, low-cost, and optimized** for IoT and AI integration ‚Äî suitable for real-time assistive applications and efficient embedded operation.

---

## üìè Success Metrics
- **Functional Accuracy:** ‚â•90% accuracy in object detection and gesture recognition  
- **User Testing:** ‚â•80% positive user feedback from trial participants  
- **System Reliability:** Stable wireless connectivity between glasses, gloves, and mobile app  

---

## üë• Roles & Contributions
All team members collaboratively contributed to all aspects of the project ‚Äî from feature development and AI model design to interface creation and documentation.

| Member | ID | Role | Responsibilities |
|---------|----|------|------------------|
| **Hana Amr** | 202200442 | AI & Hardware Engineer | Leads hardware integration and AI deployment on embedded systems |
| **Rudaina Haitham** | 202200961 | AI Engineer | Develops and trains AI models for assistive functionalities |
| **Youssef Ahmed Abdelmawla** | 202201455 | Software Engineer | Develops software systems, mobile app, and device communication logic |

This collaborative approach ensured equal participation, knowledge sharing, and consistent project quality across all stages.

---

## ‚öñÔ∏è License
This project is licensed under the [MIT License](LICENSE).

---

## üì´ Contact
For inquiries or collaboration:
- **Hana Amr** ‚Äì s-hana.mohamed@zewailcity.edu.eg  
- **Rudaina Haitham** ‚Äì s-rudaina.salmin@zewailcity.edu.eg  
- **Youssef Ahmed Abdelmawla** ‚Äì s-youssef.elmawla@zewailcity.edu.eg
